{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Here are the contents of the documentation's Homepage.</p>"},{"location":"notebooks/stock-prediction/","title":"Stock prediction","text":"<pre><code>Training set shape == (3481, 9)\nAll timestamps == 3481\nFeatured selected: [&amp;#39;&amp;lt;HIGH&amp;gt;&amp;#39;, &amp;#39;&amp;lt;LOW&amp;gt;&amp;#39;, &amp;#39;&amp;lt;CLOSE&amp;gt;&amp;#39;, &amp;#39;&amp;lt;OPEN&amp;gt;&amp;#39;, &amp;#39;&amp;lt;LAST&amp;gt;&amp;#39;, &amp;#39;&amp;lt;VOL&amp;gt;&amp;#39;]\n\n\nShape of training set == (3481, 6).\n\n\n\n\n\narray([[   4040.,    4000.,    4020.,    4001.,    4020., 1043051.],\n       [   4221.,    4221.,    4221.,    4020.,    4221., 2310692.],\n       [   4432.,    4432.,    4432.,    4221.,    4432., 1497896.],\n       ...,\n       [  32930.,   30670.,   31290.,   32280.,   31640.,      nan],\n       [  32500.,   30000.,   30900.,   31290.,   30190.,      nan],\n       [  30990.,   29360.,   29930.,   30900.,   29870.,      nan]])\n\n\n\n\n\n\narray([[-0.58738015],\n       [-0.56397588],\n       [-0.53669245],\n       ...,\n       [ 3.14825136],\n       [ 3.09265006],\n       [ 2.89739898]])\n</code></pre>"},{"location":"notebooks/stock-prediction/#creating-a-data-structure-with-90-timestamps-and-1-output","title":"Creating a data structure with 90 timestamps and 1 output","text":""},{"location":"notebooks/stock-prediction/#number-of-days-we-want-top-predict-into-the-future-is-90","title":"Number of days we want top predict into the future is 90","text":""},{"location":"notebooks/stock-prediction/#number-of-past-days-we-want-to-use-to-predict-the-future-is-60","title":"Number of past days we want to use to predict the future is 60","text":"<pre><code>X_train shape == (3332, 60, 5).\ny_train shape == (3332, 1).\n\n\nEpoch 1/30\n 2/11 [====&amp;gt;.........................] - ETA: 44s - loss: 0.4930WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2214s vs `on_train_batch_end` time: 9.6660s). Check your callbacks.\n11/11 [==============================] - ETA: 0s - loss: 0.3061\nEpoch 00001: val_loss improved from inf to 1.55036, saving model to weights.h5\n11/11 [==============================] - 19s 2s/step - loss: 0.3061 - val_loss: 1.5504\nEpoch 2/30\n11/11 [==============================] - ETA: 0s - loss: 0.2370\nEpoch 00002: val_loss did not improve from 1.55036\n11/11 [==============================] - 3s 305ms/step - loss: 0.2370 - val_loss: 1.7610\nEpoch 3/30\n11/11 [==============================] - ETA: 0s - loss: 0.2153\nEpoch 00003: val_loss improved from 1.55036 to 1.39267, saving model to weights.h5\n11/11 [==============================] - 4s 375ms/step - loss: 0.2153 - val_loss: 1.3927\nEpoch 4/30\n11/11 [==============================] - ETA: 0s - loss: 0.1800\nEpoch 00004: val_loss did not improve from 1.39267\n11/11 [==============================] - 6s 530ms/step - loss: 0.1800 - val_loss: 1.4847\nEpoch 5/30\n11/11 [==============================] - ETA: 0s - loss: 0.1458\nEpoch 00005: val_loss did not improve from 1.39267\n11/11 [==============================] - 4s 358ms/step - loss: 0.1458 - val_loss: 2.0493\nEpoch 6/30\n11/11 [==============================] - ETA: 0s - loss: 0.1729\nEpoch 00006: val_loss did not improve from 1.39267\n11/11 [==============================] - 3s 291ms/step - loss: 0.1729 - val_loss: 1.4601\nEpoch 7/30\n11/11 [==============================] - ETA: 0s - loss: 0.1454\nEpoch 00007: val_loss improved from 1.39267 to 1.22334, saving model to weights.h5\n11/11 [==============================] - 3s 274ms/step - loss: 0.1454 - val_loss: 1.2233\nEpoch 8/30\n11/11 [==============================] - ETA: 0s - loss: 0.1378\nEpoch 00008: val_loss did not improve from 1.22334\n11/11 [==============================] - 3s 309ms/step - loss: 0.1378 - val_loss: 1.3002\nEpoch 9/30\n11/11 [==============================] - ETA: 0s - loss: 0.1306\nEpoch 00009: val_loss did not improve from 1.22334\n11/11 [==============================] - 4s 319ms/step - loss: 0.1306 - val_loss: 1.3677\nEpoch 10/30\n11/11 [==============================] - ETA: 0s - loss: 0.2041\nEpoch 00010: val_loss did not improve from 1.22334\n11/11 [==============================] - 4s 402ms/step - loss: 0.2041 - val_loss: 1.2787\nEpoch 11/30\n11/11 [==============================] - ETA: 0s - loss: 0.1911\nEpoch 00011: val_loss did not improve from 1.22334\n11/11 [==============================] - 4s 330ms/step - loss: 0.1911 - val_loss: 1.3674\nEpoch 12/30\n11/11 [==============================] - ETA: 0s - loss: 0.1513\nEpoch 00012: val_loss improved from 1.22334 to 1.00346, saving model to weights.h5\n11/11 [==============================] - 4s 360ms/step - loss: 0.1513 - val_loss: 1.0035\nEpoch 13/30\n11/11 [==============================] - ETA: 0s - loss: 0.1310\nEpoch 00013: val_loss did not improve from 1.00346\n11/11 [==============================] - 4s 337ms/step - loss: 0.1310 - val_loss: 1.2693\nEpoch 14/30\n11/11 [==============================] - ETA: 0s - loss: 0.1161\nEpoch 00014: val_loss did not improve from 1.00346\n11/11 [==============================] - 4s 337ms/step - loss: 0.1161 - val_loss: 1.3064\nEpoch 15/30\n11/11 [==============================] - ETA: 0s - loss: 0.1002\nEpoch 00015: val_loss did not improve from 1.00346\n11/11 [==============================] - 3s 248ms/step - loss: 0.1002 - val_loss: 1.2901\nEpoch 16/30\n11/11 [==============================] - ETA: 0s - loss: 0.0982\nEpoch 00016: val_loss did not improve from 1.00346\n11/11 [==============================] - 3s 230ms/step - loss: 0.0982 - val_loss: 1.2895\nEpoch 17/30\n11/11 [==============================] - ETA: 0s - loss: 0.0932\nEpoch 00017: val_loss did not improve from 1.00346\n11/11 [==============================] - 2s 221ms/step - loss: 0.0932 - val_loss: 1.2538\nEpoch 18/30\n11/11 [==============================] - ETA: 0s - loss: 0.1043\nEpoch 00018: val_loss did not improve from 1.00346\n11/11 [==============================] - 2s 211ms/step - loss: 0.1043 - val_loss: 1.2111\nEpoch 19/30\n11/11 [==============================] - ETA: 0s - loss: 0.1151\nEpoch 00019: val_loss did not improve from 1.00346\n11/11 [==============================] - 2s 202ms/step - loss: 0.1151 - val_loss: 1.3723\nEpoch 20/30\n11/11 [==============================] - ETA: 0s - loss: 0.1126\nEpoch 00020: val_loss did not improve from 1.00346\n11/11 [==============================] - 2s 208ms/step - loss: 0.1126 - val_loss: 1.2194\nEpoch 21/30\n11/11 [==============================] - ETA: 0s - loss: 0.1009\nEpoch 00021: val_loss did not improve from 1.00346\n11/11 [==============================] - 2s 199ms/step - loss: 0.1009 - val_loss: 1.2398\nEpoch 22/30\n11/11 [==============================] - ETA: 0s - loss: 0.1007\nEpoch 00022: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n\nEpoch 00022: val_loss did not improve from 1.00346\n11/11 [==============================] - 2s 208ms/step - loss: 0.1007 - val_loss: 1.2714\nEpoch 00022: early stopping\nWall time: 1min 59s\n</code></pre>       &lt;OPEN&gt;     2002-08-18 5883.375977   2002-08-19 5848.064941   2002-08-24 5802.728516     <pre><code>array([[-0.34902173],\n       [-0.3535877 ],\n       [-0.3594499 ],\n       ...,\n       [ 0.02020596],\n       [ 0.01923126],\n       [ 0.01786958]], dtype=float32)\n\n\n\n\n\n\narray([[-0.33394167],\n       [-0.33122625],\n       [-0.30743407],\n       ...,\n       [ 3.14825136],\n       [ 3.09265006],\n       [ 2.89739898]])\n</code></pre>  <pre><code>Mean Absolute Error: 0.3041943613825069\nMean Squared Error: 0.31919656165415916\nRoot Mean Squared Error: 0.5649748327617428\nR2 Scrore: 0.6963427455406279\n</code></pre>"}]}